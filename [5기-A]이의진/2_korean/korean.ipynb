{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kdtai-2.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download - c kdtai-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kdtai-2.zip\n",
      "  inflating: dataset/dataset/all_zero_submission.csv  \n",
      "  inflating: dataset/dataset/random_submission.csv  \n",
      "  inflating: dataset/dataset/submission.csv  \n",
      "  inflating: dataset/dataset/test.csv  \n",
      "  inflating: dataset/dataset/train.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip kdtai-2.zip - d dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: joblib in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from soynlp) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from soynlp) (1.24.2)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-macosx_10_9_x86_64.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from soynlp) (1.10.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/lee/kdt-ai-course/KDT_AI/.conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->soynlp) (1.2.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, soynlp\n",
      "Successfully installed scikit-learn-1.2.2 soynlp-0.0.493 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install gensim\n",
    "%pip install soynlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"mps\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from nltk.tokenize import word_tokenize\n",
    "from soynlp.hangle import decompose, character_is_korean\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Dataset_type(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "\n",
    "class Korean_dataset(Dataset):\n",
    "    def __init__(self, file_path, dataset_type: Dataset_type, model, is_split_jamo=False):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        # self.transform = transform\n",
    "        self.dataset_type = dataset_type\n",
    "        self.data_df = pd.read_csv(self.file_path)\n",
    "        self.model = model\n",
    "        self.is_split_jamo = is_split_jamo\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self._embedding(self.data_df.loc[idx, \"text\"])\n",
    "\n",
    "        if self.dataset_type == Dataset_type.TRAIN:\n",
    "            trg = self.data_df.loc[idx, \"label\"]\n",
    "            return src, trg\n",
    "        else:\n",
    "            return src\n",
    "\n",
    "    def _embedding(self, text):\n",
    "        text = self._remove_special_characters(text)\n",
    "        src = word_tokenize(text)\n",
    "        src = [self._split_jamo(word)\n",
    "               for word in src] if self.is_split_jamo else src\n",
    "        src = [self.model.wv[word] for word in src]\n",
    "        src = torch.to_tensor(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "    def _remove_special_characters(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def _split_jamo(self, word):\n",
    "\n",
    "        def transform(char):\n",
    "            if char == ' ':\n",
    "                return char\n",
    "            cjj = decompose(char)\n",
    "            if len(cjj) == 1:\n",
    "                return cjj\n",
    "            cjj_ = ''.join(c if c != ' ' else '-' for c in cjj)\n",
    "            return cjj_\n",
    "\n",
    "        sent_ = []\n",
    "        for char in word:\n",
    "            if character_is_korean(char):\n",
    "                sent_.append(transform(char))\n",
    "            else:\n",
    "                sent_.append(char)\n",
    "        doublespace_pattern = re.compile('\\s+')\n",
    "        sent_ = doublespace_pattern.sub(' ', ''.join(sent_))\n",
    "        return sent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "current_path = os.getcwd()\n",
    "model_file_path = os.path.join(current_path, \"embedding_model\", \"wiki.ko.bin\")\n",
    "model = gensim.models.fasttext.load_facebook_model(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59276\n",
      "6587\n",
      "13491\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "current_path = os.getcwd()\n",
    "train_file_path = os.path.join(current_path, \"dataset\", \"train.csv\")\n",
    "test_file_path = os.path.join(current_path, \"dataset\", \"test.csv\")\n",
    "\n",
    "train_set = Korean_dataset(file_path=train_file_path,\n",
    "                           dataset_type=Dataset_type.TRAIN,\n",
    "                           is_split_jamo=True, model=model)\n",
    "test_set = Korean_dataset(file_path=test_file_path,\n",
    "                          dataset_type=Dataset_type.TEST,\n",
    "                          is_split_jamo=True, model=model)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_valid_ratio = 0.9\n",
    "train_set_count = int(len(train_set) * train_valid_ratio)\n",
    "val_set_count = len(train_set) - train_set_count\n",
    "train_set, val_set = random_split(train_set, [train_set_count, val_set_count])\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set[\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_set[1][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
